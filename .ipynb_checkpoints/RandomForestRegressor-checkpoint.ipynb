{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\cesar\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0          11 KB  conda-forge\n",
      "    conda-4.10.1               |   py38haa244fe_0         3.1 MB  conda-forge\n",
      "    libxgboost-1.4.0           |       h0e60522_0         2.2 MB  conda-forge\n",
      "    py-xgboost-1.4.0           |   py38haa244fe_0         141 KB  conda-forge\n",
      "    python_abi-3.8             |           1_cp38           4 KB  conda-forge\n",
      "    xgboost-1.4.0              |   py38haa244fe_0          11 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         5.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _py-xgboost-mutex  conda-forge/win-64::_py-xgboost-mutex-2.0-cpu_0\n",
      "  libxgboost         conda-forge/win-64::libxgboost-1.4.0-h0e60522_0\n",
      "  py-xgboost         conda-forge/win-64::py-xgboost-1.4.0-py38haa244fe_0\n",
      "  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38\n",
      "  xgboost            conda-forge/win-64::xgboost-1.4.0-py38haa244fe_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda              pkgs/main::conda-4.10.1-py38haa95532_1 --> conda-forge::conda-4.10.1-py38haa244fe_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "conda-4.10.1         | 3.1 MB    |            |   0% \n",
      "conda-4.10.1         | 3.1 MB    |            |   1% \n",
      "conda-4.10.1         | 3.1 MB    | 2          |   2% \n",
      "conda-4.10.1         | 3.1 MB    | 5          |   5% \n",
      "conda-4.10.1         | 3.1 MB    | 9          |   9% \n",
      "conda-4.10.1         | 3.1 MB    | #3         |  13% \n",
      "conda-4.10.1         | 3.1 MB    | ##3        |  24% \n",
      "conda-4.10.1         | 3.1 MB    | ###5       |  36% \n",
      "conda-4.10.1         | 3.1 MB    | ########5  |  86% \n",
      "conda-4.10.1         | 3.1 MB    | ########## | 100% \n",
      "\n",
      "py-xgboost-1.4.0     | 141 KB    |            |   0% \n",
      "py-xgboost-1.4.0     | 141 KB    | #1         |  11% \n",
      "py-xgboost-1.4.0     | 141 KB    | ########## | 100% \n",
      "\n",
      "xgboost-1.4.0        | 11 KB     |            |   0% \n",
      "xgboost-1.4.0        | 11 KB     | ########## | 100% \n",
      "xgboost-1.4.0        | 11 KB     | ########## | 100% \n",
      "\n",
      "libxgboost-1.4.0     | 2.2 MB    |            |   0% \n",
      "libxgboost-1.4.0     | 2.2 MB    |            |   1% \n",
      "libxgboost-1.4.0     | 2.2 MB    | 5          |   6% \n",
      "libxgboost-1.4.0     | 2.2 MB    | #3         |  14% \n",
      "libxgboost-1.4.0     | 2.2 MB    | ###8       |  38% \n",
      "libxgboost-1.4.0     | 2.2 MB    | ######5    |  65% \n",
      "libxgboost-1.4.0     | 2.2 MB    | ########## | 100% \n",
      "libxgboost-1.4.0     | 2.2 MB    | ########## | 100% \n",
      "\n",
      "python_abi-3.8       | 4 KB      |            |   0% \n",
      "python_abi-3.8       | 4 KB      | ########## | 100% \n",
      "python_abi-3.8       | 4 KB      | ########## | 100% \n",
      "\n",
      "_py-xgboost-mutex-2. | 11 KB     |            |   0% \n",
      "_py-xgboost-mutex-2. | 11 KB     | ########## | 100% \n",
      "_py-xgboost-mutex-2. | 11 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install -yc conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Una Lectura completa de la data se nos hace bien difícil en una PC casual que no tenga tanta ram o memoria suficiente también influye el procesador, por lo que se optara por la lectura usando chunks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk dataframe 1 size acumulado: 0.0108 GB\n",
      "Chunk dataframe 2 size acumulado: 0.0216 GB\n",
      "Chunk dataframe 3 size acumulado: 0.0324 GB\n",
      "Chunk dataframe 4 size acumulado: 0.0432 GB\n",
      "Chunk dataframe 5 size acumulado: 0.0540 GB\n",
      "Chunk dataframe 6 size acumulado: 0.0648 GB\n",
      "Chunk dataframe 7 size acumulado: 0.0756 GB\n",
      "Chunk dataframe 8 size acumulado: 0.0864 GB\n",
      "Chunk dataframe 9 size acumulado: 0.0972 GB\n",
      "Chunk dataframe 10 size acumulado: 0.1080 GB\n",
      "Chunk dataframe 11 size acumulado: 0.1188 GB\n",
      "Chunk dataframe 12 size acumulado: 0.1296 GB\n",
      "Chunk dataframe 13 size acumulado: 0.1404 GB\n",
      "Chunk dataframe 14 size acumulado: 0.1512 GB\n",
      "Chunk dataframe 15 size acumulado: 0.1620 GB\n",
      "Chunk dataframe 16 size acumulado: 0.1728 GB\n",
      "Chunk dataframe 17 size acumulado: 0.1836 GB\n",
      "Chunk dataframe 18 size acumulado: 0.1944 GB\n",
      "Chunk dataframe 19 size acumulado: 0.2052 GB\n",
      "Chunk dataframe 20 size acumulado: 0.2160 GB\n",
      "Chunk dataframe 21 size acumulado: 0.2268 GB\n",
      "Chunk dataframe 22 size acumulado: 0.2376 GB\n",
      "Chunk dataframe 23 size acumulado: 0.2484 GB\n",
      "Chunk dataframe 24 size acumulado: 0.2592 GB\n",
      "Chunk dataframe 25 size acumulado: 0.2701 GB\n",
      "Chunk dataframe 26 size acumulado: 0.2809 GB\n",
      "Chunk dataframe 27 size acumulado: 0.2917 GB\n",
      "Chunk dataframe 28 size acumulado: 0.3025 GB\n",
      "Chunk dataframe 29 size acumulado: 0.3133 GB\n",
      "Chunk dataframe 30 size acumulado: 0.3241 GB\n",
      "Chunk dataframe 31 size acumulado: 0.3349 GB\n",
      "Chunk dataframe 32 size acumulado: 0.3457 GB\n",
      "Chunk dataframe 33 size acumulado: 0.3565 GB\n",
      "Chunk dataframe 34 size acumulado: 0.3673 GB\n",
      "Chunk dataframe 35 size acumulado: 0.3781 GB\n",
      "Chunk dataframe 36 size acumulado: 0.3889 GB\n",
      "Chunk dataframe 37 size acumulado: 0.3997 GB\n",
      "Chunk dataframe 38 size acumulado: 0.4105 GB\n",
      "Chunk dataframe 39 size acumulado: 0.4213 GB\n",
      "Chunk dataframe 40 size acumulado: 0.4321 GB\n",
      "Chunk dataframe 41 size acumulado: 0.4429 GB\n",
      "Chunk dataframe 42 size acumulado: 0.4537 GB\n",
      "Chunk dataframe 43 size acumulado: 0.4645 GB\n",
      "Chunk dataframe 44 size acumulado: 0.4753 GB\n",
      "Chunk dataframe 45 size acumulado: 0.4861 GB\n",
      "Chunk dataframe 46 size acumulado: 0.4969 GB\n",
      "Chunk dataframe 47 size acumulado: 0.5077 GB\n",
      "Chunk dataframe 48 size acumulado: 0.5185 GB\n",
      "Chunk dataframe 49 size acumulado: 0.5293 GB\n",
      "Chunk dataframe 50 size acumulado: 0.5401 GB\n",
      "Chunk dataframe 51 size acumulado: 0.5509 GB\n",
      "Chunk dataframe 52 size acumulado: 0.5617 GB\n",
      "Chunk dataframe 53 size acumulado: 0.5725 GB\n",
      "Chunk dataframe 54 size acumulado: 0.5833 GB\n",
      "Chunk dataframe 55 size acumulado: 0.5941 GB\n",
      "Chunk dataframe 56 size acumulado: 0.6049 GB\n",
      "Chunk dataframe 57 size acumulado: 0.6157 GB\n",
      "Chunk dataframe 58 size acumulado: 0.6265 GB\n",
      "Chunk dataframe 59 size acumulado: 0.6373 GB\n",
      "Chunk dataframe 60 size acumulado: 0.6481 GB\n",
      "Chunk dataframe 61 size acumulado: 0.6589 GB\n",
      "Chunk dataframe 62 size acumulado: 0.6697 GB\n",
      "Chunk dataframe 63 size acumulado: 0.6805 GB\n",
      "Chunk dataframe 64 size acumulado: 0.6913 GB\n",
      "Chunk dataframe 65 size acumulado: 0.7021 GB\n",
      "Chunk dataframe 66 size acumulado: 0.7129 GB\n",
      "Chunk dataframe 67 size acumulado: 0.7237 GB\n",
      "Chunk dataframe 68 size acumulado: 0.7345 GB\n",
      "Chunk dataframe 69 size acumulado: 0.7453 GB\n",
      "Chunk dataframe 70 size acumulado: 0.7561 GB\n",
      "Chunk dataframe 71 size acumulado: 0.7669 GB\n",
      "Chunk dataframe 72 size acumulado: 0.7777 GB\n",
      "Chunk dataframe 73 size acumulado: 0.7885 GB\n",
      "Chunk dataframe 74 size acumulado: 0.7993 GB\n",
      "Chunk dataframe 75 size acumulado: 0.8102 GB\n",
      "Chunk dataframe 76 size acumulado: 0.8210 GB\n",
      "Chunk dataframe 77 size acumulado: 0.8318 GB\n",
      "Chunk dataframe 78 size acumulado: 0.8426 GB\n",
      "Chunk dataframe 79 size acumulado: 0.8534 GB\n",
      "Chunk dataframe 80 size acumulado: 0.8642 GB\n",
      "Chunk dataframe 81 size acumulado: 0.8750 GB\n",
      "Chunk dataframe 82 size acumulado: 0.8858 GB\n",
      "Chunk dataframe 83 size acumulado: 0.8966 GB\n",
      "Chunk dataframe 84 size acumulado: 0.9074 GB\n",
      "Chunk dataframe 85 size acumulado: 0.9182 GB\n",
      "Chunk dataframe 86 size acumulado: 0.9290 GB\n",
      "Chunk dataframe 87 size acumulado: 0.9398 GB\n",
      "Chunk dataframe 88 size acumulado: 0.9506 GB\n",
      "Chunk dataframe 89 size acumulado: 0.9614 GB\n",
      "Chunk dataframe 90 size acumulado: 0.9722 GB\n",
      "Chunk dataframe 91 size acumulado: 0.9830 GB\n",
      "Chunk dataframe 92 size acumulado: 0.9938 GB\n",
      "Chunk dataframe 93 size acumulado: 1.0046 GB\n",
      "Chunk dataframe 94 size acumulado: 1.0154 GB\n",
      "Chunk dataframe 95 size acumulado: 1.0262 GB\n",
      "Chunk dataframe 96 size acumulado: 1.0370 GB\n",
      "Chunk dataframe 97 size acumulado: 1.0478 GB\n",
      "Chunk dataframe 98 size acumulado: 1.0586 GB\n",
      "Chunk dataframe 99 size acumulado: 1.0694 GB\n",
      "Chunk dataframe 100 size acumulado: 1.0802 GB\n",
      "Chunk dataframe 101 size acumulado: 1.0910 GB\n",
      "Chunk dataframe 102 size acumulado: 1.1018 GB\n",
      "Chunk dataframe 103 size acumulado: 1.1126 GB\n",
      "Chunk dataframe 104 size acumulado: 1.1234 GB\n",
      "Chunk dataframe 105 size acumulado: 1.1342 GB\n",
      "Chunk dataframe 106 size acumulado: 1.1450 GB\n",
      "Chunk dataframe 107 size acumulado: 1.1558 GB\n",
      "Chunk dataframe 108 size acumulado: 1.1666 GB\n",
      "Chunk dataframe 109 size acumulado: 1.1774 GB\n",
      "Chunk dataframe 110 size acumulado: 1.1882 GB\n",
      "Chunk dataframe 111 size acumulado: 1.1990 GB\n",
      "Chunk dataframe 112 size acumulado: 1.2098 GB\n",
      "Chunk dataframe 113 size acumulado: 1.2206 GB\n",
      "Chunk dataframe 114 size acumulado: 1.2314 GB\n",
      "Chunk dataframe 115 size acumulado: 1.2422 GB\n",
      "Chunk dataframe 116 size acumulado: 1.2530 GB\n",
      "Chunk dataframe 117 size acumulado: 1.2638 GB\n",
      "Chunk dataframe 118 size acumulado: 1.2746 GB\n",
      "Chunk dataframe 119 size acumulado: 1.2854 GB\n",
      "Chunk dataframe 120 size acumulado: 1.2962 GB\n",
      "Chunk dataframe 121 size acumulado: 1.3070 GB\n",
      "Chunk dataframe 122 size acumulado: 1.3178 GB\n",
      "Chunk dataframe 123 size acumulado: 1.3286 GB\n",
      "Chunk dataframe 124 size acumulado: 1.3395 GB\n",
      "Chunk dataframe 125 size acumulado: 1.3503 GB\n",
      "Chunk dataframe 126 size acumulado: 1.3611 GB\n",
      "Chunk dataframe 127 size acumulado: 1.3719 GB\n",
      "Chunk dataframe 128 size acumulado: 1.3827 GB\n",
      "Chunk dataframe 129 size acumulado: 1.3935 GB\n",
      "Chunk dataframe 130 size acumulado: 1.4043 GB\n",
      "Chunk dataframe 131 size acumulado: 1.4151 GB\n",
      "Chunk dataframe 132 size acumulado: 1.4259 GB\n",
      "Chunk dataframe 133 size acumulado: 1.4367 GB\n",
      "Chunk dataframe 134 size acumulado: 1.4475 GB\n",
      "Chunk dataframe 135 size acumulado: 1.4583 GB\n",
      "Chunk dataframe 136 size acumulado: 1.4691 GB\n",
      "Chunk dataframe 137 size acumulado: 1.4799 GB\n",
      "Chunk dataframe 138 size acumulado: 1.4907 GB\n",
      "Chunk dataframe 139 size acumulado: 1.5015 GB\n",
      "Chunk dataframe 140 size acumulado: 1.5123 GB\n",
      "Chunk dataframe 141 size acumulado: 1.5231 GB\n",
      "Chunk dataframe 142 size acumulado: 1.5339 GB\n",
      "Chunk dataframe 143 size acumulado: 1.5447 GB\n",
      "Chunk dataframe 144 size acumulado: 1.5555 GB\n",
      "Chunk dataframe 145 size acumulado: 1.5663 GB\n",
      "Chunk dataframe 146 size acumulado: 1.5771 GB\n",
      "Chunk dataframe 147 size acumulado: 1.5879 GB\n",
      "Chunk dataframe 148 size acumulado: 1.5987 GB\n",
      "Chunk dataframe 149 size acumulado: 1.6095 GB\n",
      "Chunk dataframe 150 size acumulado: 1.6203 GB\n",
      "Chunk dataframe 151 size acumulado: 1.6311 GB\n",
      "Chunk dataframe 152 size acumulado: 1.6419 GB\n",
      "Chunk dataframe 153 size acumulado: 1.6527 GB\n",
      "Chunk dataframe 154 size acumulado: 1.6635 GB\n",
      "Chunk dataframe 155 size acumulado: 1.6743 GB\n",
      "Chunk dataframe 156 size acumulado: 1.6851 GB\n",
      "Chunk dataframe 157 size acumulado: 1.6959 GB\n",
      "Chunk dataframe 158 size acumulado: 1.7067 GB\n",
      "Chunk dataframe 159 size acumulado: 1.7175 GB\n",
      "Chunk dataframe 160 size acumulado: 1.7283 GB\n",
      "Chunk dataframe 161 size acumulado: 1.7391 GB\n",
      "Chunk dataframe 162 size acumulado: 1.7499 GB\n",
      "Chunk dataframe 163 size acumulado: 1.7607 GB\n",
      "Chunk dataframe 164 size acumulado: 1.7715 GB\n",
      "Chunk dataframe 165 size acumulado: 1.7823 GB\n",
      "Chunk dataframe 166 size acumulado: 1.7931 GB\n",
      "Chunk dataframe 167 size acumulado: 1.8039 GB\n",
      "Chunk dataframe 168 size acumulado: 1.8147 GB\n",
      "Chunk dataframe 169 size acumulado: 1.8255 GB\n",
      "Chunk dataframe 170 size acumulado: 1.8363 GB\n",
      "Chunk dataframe 171 size acumulado: 1.8471 GB\n",
      "Chunk dataframe 172 size acumulado: 1.8579 GB\n",
      "Chunk dataframe 173 size acumulado: 1.8688 GB\n",
      "Chunk dataframe 174 size acumulado: 1.8796 GB\n",
      "Chunk dataframe 175 size acumulado: 1.8904 GB\n",
      "Chunk dataframe 176 size acumulado: 1.9012 GB\n",
      "Chunk dataframe 177 size acumulado: 1.9120 GB\n",
      "Chunk dataframe 178 size acumulado: 1.9228 GB\n",
      "Chunk dataframe 179 size acumulado: 1.9336 GB\n",
      "Chunk dataframe 180 size acumulado: 1.9444 GB\n",
      "Chunk dataframe 181 size acumulado: 1.9552 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk dataframe 182 size acumulado: 1.9660 GB\n",
      "Chunk dataframe 183 size acumulado: 1.9768 GB\n",
      "Chunk dataframe 184 size acumulado: 1.9876 GB\n",
      "Chunk dataframe 185 size acumulado: 1.9984 GB\n",
      "Chunk dataframe 186 size acumulado: 2.0092 GB\n",
      "Chunk dataframe 187 size acumulado: 2.0200 GB\n",
      "Chunk dataframe 188 size acumulado: 2.0308 GB\n",
      "Chunk dataframe 189 size acumulado: 2.0416 GB\n",
      "Chunk dataframe 190 size acumulado: 2.0524 GB\n",
      "Chunk dataframe 191 size acumulado: 2.0632 GB\n",
      "Chunk dataframe 192 size acumulado: 2.0740 GB\n",
      "Chunk dataframe 193 size acumulado: 2.0848 GB\n",
      "Chunk dataframe 194 size acumulado: 2.0956 GB\n",
      "Chunk dataframe 195 size acumulado: 2.1064 GB\n",
      "Chunk dataframe 196 size acumulado: 2.1172 GB\n",
      "Chunk dataframe 197 size acumulado: 2.1280 GB\n",
      "Chunk dataframe 198 size acumulado: 2.1388 GB\n",
      "Chunk dataframe 199 size acumulado: 2.1496 GB\n",
      "Chunk dataframe 200 size acumulado: 2.1604 GB\n",
      "Chunk dataframe 201 size acumulado: 2.1712 GB\n",
      "Chunk dataframe 202 size acumulado: 2.1820 GB\n",
      "Chunk dataframe 203 size acumulado: 2.1928 GB\n",
      "Chunk dataframe 204 size acumulado: 2.2036 GB\n",
      "Chunk dataframe 205 size acumulado: 2.2144 GB\n",
      "Chunk dataframe 206 size acumulado: 2.2252 GB\n",
      "Chunk dataframe 207 size acumulado: 2.2360 GB\n",
      "Chunk dataframe 208 size acumulado: 2.2468 GB\n",
      "Chunk dataframe 209 size acumulado: 2.2576 GB\n",
      "Chunk dataframe 210 size acumulado: 2.2684 GB\n",
      "Chunk dataframe 211 size acumulado: 2.2792 GB\n",
      "Chunk dataframe 212 size acumulado: 2.2900 GB\n",
      "Chunk dataframe 213 size acumulado: 2.3008 GB\n",
      "Chunk dataframe 214 size acumulado: 2.3116 GB\n",
      "Chunk dataframe 215 size acumulado: 2.3224 GB\n",
      "Chunk dataframe 216 size acumulado: 2.3332 GB\n",
      "Chunk dataframe 217 size acumulado: 2.3440 GB\n",
      "Chunk dataframe 218 size acumulado: 2.3548 GB\n",
      "Chunk dataframe 219 size acumulado: 2.3656 GB\n",
      "Chunk dataframe 220 size acumulado: 2.3764 GB\n",
      "Chunk dataframe 221 size acumulado: 2.3872 GB\n",
      "Chunk dataframe 222 size acumulado: 2.3980 GB\n",
      "Chunk dataframe 223 size acumulado: 2.4089 GB\n",
      "Chunk dataframe 224 size acumulado: 2.4197 GB\n",
      "Chunk dataframe 225 size acumulado: 2.4305 GB\n",
      "Chunk dataframe 226 size acumulado: 2.4413 GB\n",
      "Chunk dataframe 227 size acumulado: 2.4521 GB\n",
      "Chunk dataframe 228 size acumulado: 2.4629 GB\n",
      "Chunk dataframe 229 size acumulado: 2.4737 GB\n",
      "Chunk dataframe 230 size acumulado: 2.4845 GB\n",
      "Chunk dataframe 231 size acumulado: 2.4953 GB\n",
      "Chunk dataframe 232 size acumulado: 2.5061 GB\n",
      "Chunk dataframe 233 size acumulado: 2.5169 GB\n",
      "Chunk dataframe 234 size acumulado: 2.5277 GB\n",
      "Chunk dataframe 235 size acumulado: 2.5385 GB\n",
      "Chunk dataframe 236 size acumulado: 2.5493 GB\n",
      "Chunk dataframe 237 size acumulado: 2.5601 GB\n",
      "Chunk dataframe 238 size acumulado: 2.5709 GB\n",
      "Chunk dataframe 239 size acumulado: 2.5817 GB\n",
      "Chunk dataframe 240 size acumulado: 2.5925 GB\n",
      "Chunk dataframe 241 size acumulado: 2.6033 GB\n",
      "Chunk dataframe 242 size acumulado: 2.6141 GB\n",
      "Chunk dataframe 243 size acumulado: 2.6249 GB\n",
      "Chunk dataframe 244 size acumulado: 2.6357 GB\n",
      "Chunk dataframe 245 size acumulado: 2.6465 GB\n",
      "Chunk dataframe 246 size acumulado: 2.6573 GB\n",
      "Chunk dataframe 247 size acumulado: 2.6681 GB\n",
      "Chunk dataframe 248 size acumulado: 2.6789 GB\n",
      "Chunk dataframe 249 size acumulado: 2.6897 GB\n",
      "Chunk dataframe 250 size acumulado: 2.7005 GB\n",
      "Chunk dataframe 251 size acumulado: 2.7113 GB\n",
      "Chunk dataframe 252 size acumulado: 2.7221 GB\n",
      "Chunk dataframe 253 size acumulado: 2.7329 GB\n",
      "Chunk dataframe 254 size acumulado: 2.7437 GB\n",
      "Chunk dataframe 255 size acumulado: 2.7545 GB\n",
      "Chunk dataframe 256 size acumulado: 2.7653 GB\n",
      "Chunk dataframe 257 size acumulado: 2.7761 GB\n",
      "Chunk dataframe 258 size acumulado: 2.7869 GB\n",
      "Chunk dataframe 259 size acumulado: 2.7977 GB\n",
      "Chunk dataframe 260 size acumulado: 2.8085 GB\n",
      "Chunk dataframe 261 size acumulado: 2.8193 GB\n",
      "Chunk dataframe 262 size acumulado: 2.8301 GB\n",
      "Chunk dataframe 263 size acumulado: 2.8409 GB\n",
      "Chunk dataframe 264 size acumulado: 2.8517 GB\n",
      "Chunk dataframe 265 size acumulado: 2.8625 GB\n",
      "Chunk dataframe 266 size acumulado: 2.8733 GB\n",
      "Chunk dataframe 267 size acumulado: 2.8841 GB\n",
      "Chunk dataframe 268 size acumulado: 2.8949 GB\n",
      "Chunk dataframe 269 size acumulado: 2.9057 GB\n",
      "Chunk dataframe 270 size acumulado: 2.9165 GB\n",
      "Chunk dataframe 271 size acumulado: 2.9273 GB\n",
      "Chunk dataframe 272 size acumulado: 2.9382 GB\n",
      "Chunk dataframe 273 size acumulado: 2.9490 GB\n",
      "Chunk dataframe 274 size acumulado: 2.9598 GB\n",
      "Chunk dataframe 275 size acumulado: 2.9706 GB\n",
      "Chunk dataframe 276 size acumulado: 2.9814 GB\n",
      "Chunk dataframe 277 size acumulado: 2.9922 GB\n",
      "Chunk dataframe 278 size acumulado: 3.0030 GB\n",
      "Chunk dataframe 279 size acumulado: 3.0138 GB\n",
      "Chunk dataframe 280 size acumulado: 3.0246 GB\n",
      "Chunk dataframe 281 size acumulado: 3.0354 GB\n",
      "Chunk dataframe 282 size acumulado: 3.0462 GB\n",
      "Chunk dataframe 283 size acumulado: 3.0570 GB\n",
      "Chunk dataframe 284 size acumulado: 3.0678 GB\n",
      "Chunk dataframe 285 size acumulado: 3.0786 GB\n",
      "Chunk dataframe 286 size acumulado: 3.0894 GB\n",
      "Chunk dataframe 287 size acumulado: 3.1002 GB\n",
      "Chunk dataframe 288 size acumulado: 3.1110 GB\n",
      "Chunk dataframe 289 size acumulado: 3.1218 GB\n",
      "Chunk dataframe 290 size acumulado: 3.1326 GB\n",
      "Chunk dataframe 291 size acumulado: 3.1434 GB\n",
      "Chunk dataframe 292 size acumulado: 3.1542 GB\n",
      "Chunk dataframe 293 size acumulado: 3.1650 GB\n",
      "Chunk dataframe 294 size acumulado: 3.1758 GB\n",
      "Chunk dataframe 295 size acumulado: 3.1866 GB\n",
      "Chunk dataframe 296 size acumulado: 3.1974 GB\n",
      "Chunk dataframe 297 size acumulado: 3.2082 GB\n",
      "Chunk dataframe 298 size acumulado: 3.2190 GB\n",
      "Chunk dataframe 299 size acumulado: 3.2298 GB\n",
      "Chunk dataframe 300 size acumulado: 3.2406 GB\n",
      "Chunk dataframe 301 size acumulado: 3.2514 GB\n",
      "Chunk dataframe 302 size acumulado: 3.2622 GB\n",
      "Chunk dataframe 303 size acumulado: 3.2730 GB\n",
      "Chunk dataframe 304 size acumulado: 3.2838 GB\n",
      "Chunk dataframe 305 size acumulado: 3.2946 GB\n",
      "Chunk dataframe 306 size acumulado: 3.3054 GB\n",
      "Chunk dataframe 307 size acumulado: 3.3162 GB\n",
      "Chunk dataframe 308 size acumulado: 3.3270 GB\n",
      "Chunk dataframe 309 size acumulado: 3.3378 GB\n",
      "Chunk dataframe 310 size acumulado: 3.3486 GB\n",
      "Chunk dataframe 311 size acumulado: 3.3594 GB\n",
      "Chunk dataframe 312 size acumulado: 3.3702 GB\n",
      "Chunk dataframe 313 size acumulado: 3.3810 GB\n",
      "Chunk dataframe 314 size acumulado: 3.3918 GB\n",
      "Chunk dataframe 315 size acumulado: 3.4026 GB\n",
      "Chunk dataframe 316 size acumulado: 3.4134 GB\n",
      "Chunk dataframe 317 size acumulado: 3.4242 GB\n",
      "Chunk dataframe 318 size acumulado: 3.4350 GB\n",
      "Chunk dataframe 319 size acumulado: 3.4458 GB\n",
      "Chunk dataframe 320 size acumulado: 3.4566 GB\n",
      "Chunk dataframe 321 size acumulado: 3.4674 GB\n",
      "Chunk dataframe 322 size acumulado: 3.4783 GB\n",
      "Chunk dataframe 323 size acumulado: 3.4891 GB\n",
      "Chunk dataframe 324 size acumulado: 3.4999 GB\n",
      "Chunk dataframe 325 size acumulado: 3.5107 GB\n",
      "Chunk dataframe 326 size acumulado: 3.5215 GB\n",
      "Chunk dataframe 327 size acumulado: 3.5323 GB\n",
      "Chunk dataframe 328 size acumulado: 3.5431 GB\n",
      "Chunk dataframe 329 size acumulado: 3.5539 GB\n",
      "Chunk dataframe 330 size acumulado: 3.5647 GB\n",
      "Chunk dataframe 331 size acumulado: 3.5755 GB\n",
      "Chunk dataframe 332 size acumulado: 3.5863 GB\n",
      "Chunk dataframe 333 size acumulado: 3.5971 GB\n",
      "Chunk dataframe 334 size acumulado: 3.6079 GB\n",
      "Chunk dataframe 335 size acumulado: 3.6187 GB\n",
      "Chunk dataframe 336 size acumulado: 3.6295 GB\n",
      "Chunk dataframe 337 size acumulado: 3.6403 GB\n",
      "Chunk dataframe 338 size acumulado: 3.6511 GB\n",
      "Chunk dataframe 339 size acumulado: 3.6619 GB\n",
      "Chunk dataframe 340 size acumulado: 3.6727 GB\n",
      "Chunk dataframe 341 size acumulado: 3.6835 GB\n",
      "Chunk dataframe 342 size acumulado: 3.6943 GB\n",
      "Chunk dataframe 343 size acumulado: 3.7051 GB\n",
      "Chunk dataframe 344 size acumulado: 3.7159 GB\n",
      "Chunk dataframe 345 size acumulado: 3.7267 GB\n",
      "Chunk dataframe 346 size acumulado: 3.7375 GB\n",
      "Chunk dataframe 347 size acumulado: 3.7483 GB\n",
      "Chunk dataframe 348 size acumulado: 3.7591 GB\n",
      "Chunk dataframe 349 size acumulado: 3.7699 GB\n",
      "Chunk dataframe 350 size acumulado: 3.7807 GB\n",
      "Chunk dataframe 351 size acumulado: 3.7915 GB\n",
      "Chunk dataframe 352 size acumulado: 3.8023 GB\n",
      "Chunk dataframe 353 size acumulado: 3.8131 GB\n",
      "Chunk dataframe 354 size acumulado: 3.8239 GB\n",
      "Chunk dataframe 355 size acumulado: 3.8347 GB\n",
      "Chunk dataframe 356 size acumulado: 3.8455 GB\n",
      "Chunk dataframe 357 size acumulado: 3.8563 GB\n",
      "Chunk dataframe 358 size acumulado: 3.8671 GB\n",
      "Chunk dataframe 359 size acumulado: 3.8779 GB\n",
      "Chunk dataframe 360 size acumulado: 3.8887 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk dataframe 361 size acumulado: 3.8995 GB\n",
      "Chunk dataframe 362 size acumulado: 3.9103 GB\n",
      "Chunk dataframe 363 size acumulado: 3.9211 GB\n",
      "Chunk dataframe 364 size acumulado: 3.9319 GB\n",
      "Chunk dataframe 365 size acumulado: 3.9427 GB\n",
      "Chunk dataframe 366 size acumulado: 3.9535 GB\n",
      "Chunk dataframe 367 size acumulado: 3.9643 GB\n",
      "Chunk dataframe 368 size acumulado: 3.9751 GB\n",
      "Chunk dataframe 369 size acumulado: 3.9859 GB\n",
      "Chunk dataframe 370 size acumulado: 3.9967 GB\n",
      "Chunk dataframe 371 size acumulado: 4.0076 GB\n",
      "Chunk dataframe 372 size acumulado: 4.0184 GB\n",
      "Chunk dataframe 373 size acumulado: 4.0292 GB\n",
      "Chunk dataframe 374 size acumulado: 4.0400 GB\n",
      "Chunk dataframe 375 size acumulado: 4.0508 GB\n",
      "Chunk dataframe 376 size acumulado: 4.0616 GB\n",
      "Chunk dataframe 377 size acumulado: 4.0724 GB\n",
      "Chunk dataframe 378 size acumulado: 4.0832 GB\n",
      "Chunk dataframe 379 size acumulado: 4.0940 GB\n",
      "Chunk dataframe 380 size acumulado: 4.1048 GB\n",
      "Chunk dataframe 381 size acumulado: 4.1156 GB\n",
      "Chunk dataframe 382 size acumulado: 4.1264 GB\n",
      "Chunk dataframe 383 size acumulado: 4.1372 GB\n",
      "Chunk dataframe 384 size acumulado: 4.1480 GB\n",
      "Chunk dataframe 385 size acumulado: 4.1588 GB\n",
      "Chunk dataframe 386 size acumulado: 4.1696 GB\n",
      "Chunk dataframe 387 size acumulado: 4.1804 GB\n",
      "Chunk dataframe 388 size acumulado: 4.1912 GB\n",
      "Chunk dataframe 389 size acumulado: 4.2020 GB\n",
      "Chunk dataframe 390 size acumulado: 4.2128 GB\n",
      "Chunk dataframe 391 size acumulado: 4.2236 GB\n",
      "Chunk dataframe 392 size acumulado: 4.2344 GB\n",
      "Chunk dataframe 393 size acumulado: 4.2452 GB\n",
      "Chunk dataframe 394 size acumulado: 4.2560 GB\n",
      "Chunk dataframe 395 size acumulado: 4.2668 GB\n",
      "Chunk dataframe 396 size acumulado: 4.2776 GB\n",
      "Chunk dataframe 397 size acumulado: 4.2884 GB\n",
      "Chunk dataframe 398 size acumulado: 4.2992 GB\n",
      "Chunk dataframe 399 size acumulado: 4.3100 GB\n",
      "Chunk dataframe 400 size acumulado: 4.3208 GB\n",
      "Chunk dataframe 401 size acumulado: 4.3316 GB\n",
      "Chunk dataframe 402 size acumulado: 4.3424 GB\n",
      "Chunk dataframe 403 size acumulado: 4.3532 GB\n",
      "Chunk dataframe 404 size acumulado: 4.3640 GB\n",
      "Chunk dataframe 405 size acumulado: 4.3748 GB\n",
      "Chunk dataframe 406 size acumulado: 4.3856 GB\n",
      "Chunk dataframe 407 size acumulado: 4.3964 GB\n",
      "Chunk dataframe 408 size acumulado: 4.4072 GB\n",
      "Chunk dataframe 409 size acumulado: 4.4180 GB\n",
      "Chunk dataframe 410 size acumulado: 4.4288 GB\n",
      "Chunk dataframe 411 size acumulado: 4.4396 GB\n",
      "Chunk dataframe 412 size acumulado: 4.4504 GB\n",
      "Chunk dataframe 413 size acumulado: 4.4612 GB\n",
      "Chunk dataframe 414 size acumulado: 4.4720 GB\n",
      "Chunk dataframe 415 size acumulado: 4.4828 GB\n",
      "Chunk dataframe 416 size acumulado: 4.4936 GB\n",
      "Chunk dataframe 417 size acumulado: 4.5044 GB\n",
      "Chunk dataframe 418 size acumulado: 4.5152 GB\n",
      "Chunk dataframe 419 size acumulado: 4.5260 GB\n",
      "Chunk dataframe 420 size acumulado: 4.5369 GB\n",
      "Chunk dataframe 421 size acumulado: 4.5477 GB\n",
      "Chunk dataframe 422 size acumulado: 4.5585 GB\n",
      "Chunk dataframe 423 size acumulado: 4.5693 GB\n",
      "Chunk dataframe 424 size acumulado: 4.5801 GB\n",
      "Chunk dataframe 425 size acumulado: 4.5909 GB\n",
      "Chunk dataframe 426 size acumulado: 4.6017 GB\n",
      "Chunk dataframe 427 size acumulado: 4.6125 GB\n",
      "Chunk dataframe 428 size acumulado: 4.6233 GB\n",
      "Chunk dataframe 429 size acumulado: 4.6341 GB\n",
      "Chunk dataframe 430 size acumulado: 4.6449 GB\n",
      "Chunk dataframe 431 size acumulado: 4.6557 GB\n",
      "Chunk dataframe 432 size acumulado: 4.6665 GB\n",
      "Chunk dataframe 433 size acumulado: 4.6773 GB\n",
      "Chunk dataframe 434 size acumulado: 4.6881 GB\n",
      "Chunk dataframe 435 size acumulado: 4.6989 GB\n",
      "Chunk dataframe 436 size acumulado: 4.7097 GB\n",
      "Chunk dataframe 437 size acumulado: 4.7205 GB\n",
      "Chunk dataframe 438 size acumulado: 4.7313 GB\n",
      "Chunk dataframe 439 size acumulado: 4.7421 GB\n",
      "Chunk dataframe 440 size acumulado: 4.7529 GB\n",
      "Chunk dataframe 441 size acumulado: 4.7637 GB\n",
      "Chunk dataframe 442 size acumulado: 4.7745 GB\n",
      "Chunk dataframe 443 size acumulado: 4.7853 GB\n",
      "Chunk dataframe 444 size acumulado: 4.7961 GB\n",
      "Chunk dataframe 445 size acumulado: 4.8069 GB\n",
      "Chunk dataframe 446 size acumulado: 4.8177 GB\n",
      "Chunk dataframe 447 size acumulado: 4.8285 GB\n",
      "Chunk dataframe 448 size acumulado: 4.8393 GB\n",
      "Chunk dataframe 449 size acumulado: 4.8501 GB\n",
      "Chunk dataframe 450 size acumulado: 4.8609 GB\n",
      "Chunk dataframe 451 size acumulado: 4.8717 GB\n",
      "Chunk dataframe 452 size acumulado: 4.8825 GB\n",
      "Chunk dataframe 453 size acumulado: 4.8933 GB\n",
      "Chunk dataframe 454 size acumulado: 4.9041 GB\n",
      "Chunk dataframe 455 size acumulado: 4.9149 GB\n",
      "Chunk dataframe 456 size acumulado: 4.9257 GB\n",
      "Chunk dataframe 457 size acumulado: 4.9365 GB\n",
      "Chunk dataframe 458 size acumulado: 4.9473 GB\n",
      "Chunk dataframe 459 size acumulado: 4.9581 GB\n",
      "Chunk dataframe 460 size acumulado: 4.9689 GB\n",
      "Chunk dataframe 461 size acumulado: 4.9797 GB\n",
      "Chunk dataframe 462 size acumulado: 4.9905 GB\n",
      "Chunk dataframe 463 size acumulado: 5.0013 GB\n",
      "Chunk dataframe 464 size acumulado: 5.0121 GB\n",
      "Chunk dataframe 465 size acumulado: 5.0229 GB\n",
      "Chunk dataframe 466 size acumulado: 5.0337 GB\n",
      "Chunk dataframe 467 size acumulado: 5.0445 GB\n",
      "Chunk dataframe 468 size acumulado: 5.0553 GB\n",
      "Chunk dataframe 469 size acumulado: 5.0662 GB\n",
      "Chunk dataframe 470 size acumulado: 5.0770 GB\n",
      "Chunk dataframe 471 size acumulado: 5.0878 GB\n",
      "Chunk dataframe 472 size acumulado: 5.0986 GB\n",
      "Chunk dataframe 473 size acumulado: 5.1094 GB\n",
      "Chunk dataframe 474 size acumulado: 5.1202 GB\n",
      "Chunk dataframe 475 size acumulado: 5.1310 GB\n",
      "Chunk dataframe 476 size acumulado: 5.1418 GB\n",
      "Chunk dataframe 477 size acumulado: 5.1526 GB\n",
      "Chunk dataframe 478 size acumulado: 5.1634 GB\n",
      "Chunk dataframe 479 size acumulado: 5.1742 GB\n",
      "Chunk dataframe 480 size acumulado: 5.1850 GB\n",
      "Chunk dataframe 481 size acumulado: 5.1958 GB\n",
      "Chunk dataframe 482 size acumulado: 5.2066 GB\n",
      "Chunk dataframe 483 size acumulado: 5.2174 GB\n",
      "Chunk dataframe 484 size acumulado: 5.2282 GB\n",
      "Chunk dataframe 485 size acumulado: 5.2390 GB\n",
      "Chunk dataframe 486 size acumulado: 5.2498 GB\n",
      "Chunk dataframe 487 size acumulado: 5.2606 GB\n",
      "Chunk dataframe 488 size acumulado: 5.2714 GB\n",
      "Chunk dataframe 489 size acumulado: 5.2822 GB\n",
      "Chunk dataframe 490 size acumulado: 5.2930 GB\n",
      "Chunk dataframe 491 size acumulado: 5.3038 GB\n",
      "Chunk dataframe 492 size acumulado: 5.3146 GB\n",
      "Chunk dataframe 493 size acumulado: 5.3254 GB\n",
      "Chunk dataframe 494 size acumulado: 5.3362 GB\n",
      "Chunk dataframe 495 size acumulado: 5.3470 GB\n",
      "Chunk dataframe 496 size acumulado: 5.3578 GB\n",
      "Chunk dataframe 497 size acumulado: 5.3686 GB\n",
      "Chunk dataframe 498 size acumulado: 5.3794 GB\n",
      "Chunk dataframe 499 size acumulado: 5.3902 GB\n",
      "Chunk dataframe 500 size acumulado: 5.4010 GB\n",
      "Chunk dataframe 501 size acumulado: 5.4118 GB\n",
      "Chunk dataframe 502 size acumulado: 5.4226 GB\n",
      "Chunk dataframe 503 size acumulado: 5.4334 GB\n",
      "Chunk dataframe 504 size acumulado: 5.4442 GB\n",
      "Chunk dataframe 505 size acumulado: 5.4550 GB\n",
      "Chunk dataframe 506 size acumulado: 5.4658 GB\n",
      "Chunk dataframe 507 size acumulado: 5.4766 GB\n",
      "Chunk dataframe 508 size acumulado: 5.4874 GB\n",
      "Chunk dataframe 509 size acumulado: 5.4982 GB\n",
      "Chunk dataframe 510 size acumulado: 5.5090 GB\n",
      "Chunk dataframe 511 size acumulado: 5.5198 GB\n",
      "Chunk dataframe 512 size acumulado: 5.5306 GB\n",
      "Chunk dataframe 513 size acumulado: 5.5414 GB\n",
      "Chunk dataframe 514 size acumulado: 5.5522 GB\n",
      "Chunk dataframe 515 size acumulado: 5.5630 GB\n",
      "Chunk dataframe 516 size acumulado: 5.5738 GB\n",
      "Chunk dataframe 517 size acumulado: 5.5846 GB\n",
      "Chunk dataframe 518 size acumulado: 5.5955 GB\n",
      "Chunk dataframe 519 size acumulado: 5.6063 GB\n",
      "Chunk dataframe 520 size acumulado: 5.6171 GB\n",
      "Chunk dataframe 521 size acumulado: 5.6279 GB\n",
      "Chunk dataframe 522 size acumulado: 5.6387 GB\n",
      "Chunk dataframe 523 size acumulado: 5.6495 GB\n",
      "Chunk dataframe 524 size acumulado: 5.6603 GB\n",
      "Chunk dataframe 525 size acumulado: 5.6711 GB\n",
      "Chunk dataframe 526 size acumulado: 5.6819 GB\n",
      "Chunk dataframe 527 size acumulado: 5.6927 GB\n",
      "Chunk dataframe 528 size acumulado: 5.7035 GB\n",
      "Chunk dataframe 529 size acumulado: 5.7143 GB\n",
      "Chunk dataframe 530 size acumulado: 5.7251 GB\n",
      "Chunk dataframe 531 size acumulado: 5.7359 GB\n",
      "Chunk dataframe 532 size acumulado: 5.7467 GB\n",
      "Chunk dataframe 533 size acumulado: 5.7575 GB\n",
      "Chunk dataframe 534 size acumulado: 5.7683 GB\n",
      "Chunk dataframe 535 size acumulado: 5.7791 GB\n",
      "Chunk dataframe 536 size acumulado: 5.7899 GB\n",
      "Chunk dataframe 537 size acumulado: 5.8007 GB\n",
      "Chunk dataframe 538 size acumulado: 5.8115 GB\n",
      "Chunk dataframe 539 size acumulado: 5.8223 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk dataframe 540 size acumulado: 5.8331 GB\n",
      "Chunk dataframe 541 size acumulado: 5.8439 GB\n",
      "Chunk dataframe 542 size acumulado: 5.8547 GB\n",
      "Chunk dataframe 543 size acumulado: 5.8655 GB\n",
      "Chunk dataframe 544 size acumulado: 5.8763 GB\n",
      "Chunk dataframe 545 size acumulado: 5.8871 GB\n",
      "Chunk dataframe 546 size acumulado: 5.8979 GB\n",
      "Chunk dataframe 547 size acumulado: 5.9087 GB\n",
      "Chunk dataframe 548 size acumulado: 5.9195 GB\n",
      "Chunk dataframe 549 size acumulado: 5.9303 GB\n",
      "Chunk dataframe 550 size acumulado: 5.9411 GB\n",
      "Chunk dataframe 551 size acumulado: 5.9519 GB\n",
      "Chunk dataframe 552 size acumulado: 5.9627 GB\n",
      "Chunk dataframe 553 size acumulado: 5.9735 GB\n",
      "Chunk dataframe 554 size acumulado: 5.9843 GB\n",
      "Chunk dataframe 555 size acumulado: 5.9869 GB\n"
     ]
    }
   ],
   "source": [
    "#Datos=pd.read_csv('train.csv') ->Nuestra data proporcionada\n",
    "import datetime as dt\n",
    "data_reduce=pd.DataFrame({},columns = ['fare_amount', 'pickup_datetime', 'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count'])\n",
    "TamañoChunks = 100000\n",
    "iteracion=0\n",
    "size=0\n",
    "for chunk in pd.read_csv('train.csv', chunksize=TamañoChunks):\n",
    "        chunk['pickup_datetime']=pd.to_datetime(chunk['pickup_datetime'])\n",
    "        chunk['pickup_datetime']=chunk['pickup_datetime'].map(dt.datetime.toordinal)\n",
    "        chunk['pickup_datetime']=chunk['pickup_datetime'].astype('uint32')      \n",
    "        chunk['fare_amount']=chunk['fare_amount'].astype('float32')\n",
    "        chunk['passenger_count']=chunk['passenger_count'].astype('uint8')\n",
    "        #print(getsizeof(chunk.iloc[[1,3,4,5,6,7]]))\n",
    "        \n",
    "        \n",
    "data_reduce = pd.concat([data_reduce, chunk.drop(['key'], axis=1)])\n",
    "        iteracion=iteracion+1\n",
    "        print('Chunk dataframe %d '%iteracion +'size acumulado: %2.4f GB'%(getsizeof(data_reduce)/(1024.0**3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\">Mostrando la data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>733573</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.9</td>\n",
       "      <td>733777</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7</td>\n",
       "      <td>734367</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.7</td>\n",
       "      <td>734614</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.3</td>\n",
       "      <td>733840</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423851</th>\n",
       "      <td>14.0</td>\n",
       "      <td>735307</td>\n",
       "      <td>-74.005272</td>\n",
       "      <td>40.740027</td>\n",
       "      <td>-73.963280</td>\n",
       "      <td>40.762555</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423852</th>\n",
       "      <td>4.2</td>\n",
       "      <td>733490</td>\n",
       "      <td>-73.957784</td>\n",
       "      <td>40.765530</td>\n",
       "      <td>-73.951640</td>\n",
       "      <td>40.773959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423853</th>\n",
       "      <td>14.1</td>\n",
       "      <td>734229</td>\n",
       "      <td>-73.970505</td>\n",
       "      <td>40.752325</td>\n",
       "      <td>-73.960537</td>\n",
       "      <td>40.797342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423854</th>\n",
       "      <td>28.9</td>\n",
       "      <td>734436</td>\n",
       "      <td>-73.980901</td>\n",
       "      <td>40.764629</td>\n",
       "      <td>-73.870605</td>\n",
       "      <td>40.773963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423855</th>\n",
       "      <td>7.5</td>\n",
       "      <td>735579</td>\n",
       "      <td>-73.969722</td>\n",
       "      <td>40.797668</td>\n",
       "      <td>-73.970885</td>\n",
       "      <td>40.783313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55423856 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fare_amount pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0                 4.5          733573        -73.844311        40.721319   \n",
       "1                16.9          733777        -74.016048        40.711303   \n",
       "2                 5.7          734367        -73.982738        40.761270   \n",
       "3                 7.7          734614        -73.987130        40.733143   \n",
       "4                 5.3          733840        -73.968095        40.768008   \n",
       "...               ...             ...               ...              ...   \n",
       "55423851         14.0          735307        -74.005272        40.740027   \n",
       "55423852          4.2          733490        -73.957784        40.765530   \n",
       "55423853         14.1          734229        -73.970505        40.752325   \n",
       "55423854         28.9          734436        -73.980901        40.764629   \n",
       "55423855          7.5          735579        -73.969722        40.797668   \n",
       "\n",
       "          dropoff_longitude  dropoff_latitude passenger_count  \n",
       "0                -73.841610         40.712278               1  \n",
       "1                -73.979268         40.782004               1  \n",
       "2                -73.991242         40.750562               2  \n",
       "3                -73.991567         40.758092               1  \n",
       "4                -73.956655         40.783762               1  \n",
       "...                     ...               ...             ...  \n",
       "55423851         -73.963280         40.762555               1  \n",
       "55423852         -73.951640         40.773959               1  \n",
       "55423853         -73.960537         40.797342               1  \n",
       "55423854         -73.870605         40.773963               1  \n",
       "55423855         -73.970885         40.783313               1  \n",
       "\n",
       "[55423856 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictores = ['pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduce.to_csv('Data_Reduce.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Hemos guardado la data anteriomente reducida como \"Data_Reduce\", y ahora procederemos a un correcta lecutra</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduce = pd.read_csv(\"Data_Reduce.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(data_reduce[predictores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=scaler.transform(data_reduce[predictores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_reduce['fare_amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacemos una limpieza de data, pero primero mostraremos cuantos valores NaN hay**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nan_values = data_reduce.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare_amount            0\n",
      "pickup_datetime        0\n",
      "pickup_longitude       0\n",
      "pickup_latitude        0\n",
      "dropoff_longitude    376\n",
      "dropoff_latitude     376\n",
      "passenger_count        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(total_nan_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora aplicamos dropna, para la correcta limmpieza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sin_nan = data_reduce.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora mostramos la data limpia sin valores NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare_amount          0\n",
      "pickup_datetime      0\n",
      "pickup_longitude     0\n",
      "pickup_latitude      0\n",
      "dropoff_longitude    0\n",
      "dropoff_latitude     0\n",
      "passenger_count      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data_sin_nan.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>733573</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.9</td>\n",
       "      <td>733777</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7</td>\n",
       "      <td>734367</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.7</td>\n",
       "      <td>734614</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.3</td>\n",
       "      <td>733840</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423851</th>\n",
       "      <td>14.0</td>\n",
       "      <td>735307</td>\n",
       "      <td>-74.005272</td>\n",
       "      <td>40.740027</td>\n",
       "      <td>-73.963280</td>\n",
       "      <td>40.762555</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423852</th>\n",
       "      <td>4.2</td>\n",
       "      <td>733490</td>\n",
       "      <td>-73.957784</td>\n",
       "      <td>40.765530</td>\n",
       "      <td>-73.951640</td>\n",
       "      <td>40.773959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423853</th>\n",
       "      <td>14.1</td>\n",
       "      <td>734229</td>\n",
       "      <td>-73.970505</td>\n",
       "      <td>40.752325</td>\n",
       "      <td>-73.960537</td>\n",
       "      <td>40.797342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423854</th>\n",
       "      <td>28.9</td>\n",
       "      <td>734436</td>\n",
       "      <td>-73.980901</td>\n",
       "      <td>40.764629</td>\n",
       "      <td>-73.870605</td>\n",
       "      <td>40.773963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423855</th>\n",
       "      <td>7.5</td>\n",
       "      <td>735579</td>\n",
       "      <td>-73.969722</td>\n",
       "      <td>40.797668</td>\n",
       "      <td>-73.970885</td>\n",
       "      <td>40.783313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55423480 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fare_amount pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0                 4.5          733573        -73.844311        40.721319   \n",
       "1                16.9          733777        -74.016048        40.711303   \n",
       "2                 5.7          734367        -73.982738        40.761270   \n",
       "3                 7.7          734614        -73.987130        40.733143   \n",
       "4                 5.3          733840        -73.968095        40.768008   \n",
       "...               ...             ...               ...              ...   \n",
       "55423851         14.0          735307        -74.005272        40.740027   \n",
       "55423852          4.2          733490        -73.957784        40.765530   \n",
       "55423853         14.1          734229        -73.970505        40.752325   \n",
       "55423854         28.9          734436        -73.980901        40.764629   \n",
       "55423855          7.5          735579        -73.969722        40.797668   \n",
       "\n",
       "          dropoff_longitude  dropoff_latitude passenger_count  \n",
       "0                -73.841610         40.712278               1  \n",
       "1                -73.979268         40.782004               1  \n",
       "2                -73.991242         40.750562               2  \n",
       "3                -73.991567         40.758092               1  \n",
       "4                -73.956655         40.783762               1  \n",
       "...                     ...               ...             ...  \n",
       "55423851         -73.963280         40.762555               1  \n",
       "55423852         -73.951640         40.773959               1  \n",
       "55423853         -73.960537         40.797342               1  \n",
       "55423854         -73.870605         40.773963               1  \n",
       "55423855         -73.970885         40.783313               1  \n",
       "\n",
       "[55423480 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sin_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=scaler.transform(data_sin_nan[predictores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_sin_nan['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data_Reduce.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>733573</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.9</td>\n",
       "      <td>733777</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7</td>\n",
       "      <td>734367</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.7</td>\n",
       "      <td>734614</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.3</td>\n",
       "      <td>733840</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423851</th>\n",
       "      <td>14.0</td>\n",
       "      <td>735307</td>\n",
       "      <td>-74.005272</td>\n",
       "      <td>40.740027</td>\n",
       "      <td>-73.963280</td>\n",
       "      <td>40.762555</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423852</th>\n",
       "      <td>4.2</td>\n",
       "      <td>733490</td>\n",
       "      <td>-73.957784</td>\n",
       "      <td>40.765530</td>\n",
       "      <td>-73.951640</td>\n",
       "      <td>40.773959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423853</th>\n",
       "      <td>14.1</td>\n",
       "      <td>734229</td>\n",
       "      <td>-73.970505</td>\n",
       "      <td>40.752325</td>\n",
       "      <td>-73.960537</td>\n",
       "      <td>40.797342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423854</th>\n",
       "      <td>28.9</td>\n",
       "      <td>734436</td>\n",
       "      <td>-73.980901</td>\n",
       "      <td>40.764629</td>\n",
       "      <td>-73.870605</td>\n",
       "      <td>40.773963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423855</th>\n",
       "      <td>7.5</td>\n",
       "      <td>735579</td>\n",
       "      <td>-73.969722</td>\n",
       "      <td>40.797668</td>\n",
       "      <td>-73.970885</td>\n",
       "      <td>40.783313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55423856 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fare_amount  pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0                 4.5           733573        -73.844311        40.721319   \n",
       "1                16.9           733777        -74.016048        40.711303   \n",
       "2                 5.7           734367        -73.982738        40.761270   \n",
       "3                 7.7           734614        -73.987130        40.733143   \n",
       "4                 5.3           733840        -73.968095        40.768008   \n",
       "...               ...              ...               ...              ...   \n",
       "55423851         14.0           735307        -74.005272        40.740027   \n",
       "55423852          4.2           733490        -73.957784        40.765530   \n",
       "55423853         14.1           734229        -73.970505        40.752325   \n",
       "55423854         28.9           734436        -73.980901        40.764629   \n",
       "55423855          7.5           735579        -73.969722        40.797668   \n",
       "\n",
       "          dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0                -73.841610         40.712278                1  \n",
       "1                -73.979268         40.782004                1  \n",
       "2                -73.991242         40.750562                2  \n",
       "3                -73.991567         40.758092                1  \n",
       "4                -73.956655         40.783762                1  \n",
       "...                     ...               ...              ...  \n",
       "55423851         -73.963280         40.762555                1  \n",
       "55423852         -73.951640         40.773959                1  \n",
       "55423853         -73.960537         40.797342                1  \n",
       "55423854         -73.870605         40.773963                1  \n",
       "55423855         -73.970885         40.783313                1  \n",
       "\n",
       "[55423856 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sin_nan = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>733573</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.9</td>\n",
       "      <td>733777</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7</td>\n",
       "      <td>734367</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.7</td>\n",
       "      <td>734614</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.3</td>\n",
       "      <td>733840</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423851</th>\n",
       "      <td>14.0</td>\n",
       "      <td>735307</td>\n",
       "      <td>-74.005272</td>\n",
       "      <td>40.740027</td>\n",
       "      <td>-73.963280</td>\n",
       "      <td>40.762555</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423852</th>\n",
       "      <td>4.2</td>\n",
       "      <td>733490</td>\n",
       "      <td>-73.957784</td>\n",
       "      <td>40.765530</td>\n",
       "      <td>-73.951640</td>\n",
       "      <td>40.773959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423853</th>\n",
       "      <td>14.1</td>\n",
       "      <td>734229</td>\n",
       "      <td>-73.970505</td>\n",
       "      <td>40.752325</td>\n",
       "      <td>-73.960537</td>\n",
       "      <td>40.797342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423854</th>\n",
       "      <td>28.9</td>\n",
       "      <td>734436</td>\n",
       "      <td>-73.980901</td>\n",
       "      <td>40.764629</td>\n",
       "      <td>-73.870605</td>\n",
       "      <td>40.773963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423855</th>\n",
       "      <td>7.5</td>\n",
       "      <td>735579</td>\n",
       "      <td>-73.969722</td>\n",
       "      <td>40.797668</td>\n",
       "      <td>-73.970885</td>\n",
       "      <td>40.783313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55423480 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fare_amount  pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0                 4.5           733573        -73.844311        40.721319   \n",
       "1                16.9           733777        -74.016048        40.711303   \n",
       "2                 5.7           734367        -73.982738        40.761270   \n",
       "3                 7.7           734614        -73.987130        40.733143   \n",
       "4                 5.3           733840        -73.968095        40.768008   \n",
       "...               ...              ...               ...              ...   \n",
       "55423851         14.0           735307        -74.005272        40.740027   \n",
       "55423852          4.2           733490        -73.957784        40.765530   \n",
       "55423853         14.1           734229        -73.970505        40.752325   \n",
       "55423854         28.9           734436        -73.980901        40.764629   \n",
       "55423855          7.5           735579        -73.969722        40.797668   \n",
       "\n",
       "          dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0                -73.841610         40.712278                1  \n",
       "1                -73.979268         40.782004                1  \n",
       "2                -73.991242         40.750562                2  \n",
       "3                -73.991567         40.758092                1  \n",
       "4                -73.956655         40.783762                1  \n",
       "...                     ...               ...              ...  \n",
       "55423851         -73.963280         40.762555                1  \n",
       "55423852         -73.951640         40.773959                1  \n",
       "55423853         -73.960537         40.797342                1  \n",
       "55423854         -73.870605         40.773963                1  \n",
       "55423855         -73.970885         40.783313                1  \n",
       "\n",
       "[55423480 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sin_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hecho esto, hemos delimitado la data con 30M de registros, ya que por temas de hardware no podemos correr toda la data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**para delimitar la data usamos => df.iloc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(data_limitada_sin_nan[predictores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictores = ['pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=scaler.transform(data_limitada_sin_nan[predictores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_limitada_sin_nan['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8105962101592771\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.30, random_state=0)\n",
    "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)  \n",
    "regressor.fit(x_train, y_train)  \n",
    "y_pred=regressor.predict(x_test)\n",
    "score=regressor.score(x_test,y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
